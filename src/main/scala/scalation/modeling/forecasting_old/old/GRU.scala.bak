
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  John Miller, Sai
 *  @version 2.0
 *  @date    
 *  @see     LICENSE (MIT style license file).
 *
 *  @title   Gated Recurrent Unit (GRU)
 *  
 *  Translated from Matlab to Scala
 *  @see     https://www.math.ucla.edu/~minchen/doc/BPTTTutorial.pdf
 */

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/* This program tests the BPTT process we manually developed for GRU.
 * We calculate the gradients of GRU parameters with chain rule , and then
 * compare them to the numerical gradients to check whether our chain rule
 * derivation is correct.
 *
 * Here, we provided 2 versions of BPTT, backward_direct() and backward().
 * The former one is the direct idea to calculate gradient within each step
 * and add them up (O(sentence_sizeˆ2) time) . The latter one is optimized to
 * calculate the contribution of each step to the overall gradient , which is
 * only O(sentence_size) time.
 *
 * This is very helpful for people who want to implement GRU in Caffe since
 * Caffe does not support auto−differentiation. This is also very helpful for
 * the people who want to know the details about Back-propagation Through
 * Time algorithm in the Recurrent Neural Networks (such as GRU and LSTM)
 * and also get a sense on how auto−differentiation is possible.
 *
 * NOTE: We does not involve SGD training here. With SGD training, this
 * program would become a complete implementation of GRU which can be
 * trained with sequence data . However, since this is only a CPU serial
 * Matlab version of GRU, applying it on large datasets will be dramatically
 * slow.
 *
 * by Minchen Li, at The University of British Columbia. 2016−04−21
 */

package scalation
package modeling
package forecasting

import scala.math.log

import scalation.mathstat.{MatrixD, VectorD}
import scalation.random.{NormalMat, NormalVec_c}

import ActivationFun.{sigmoid_, softmax_, tanh_}
import MatrixD.outer

def log_ (x: VectorD): VectorD = x.map (log)

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `CRU` class implements Gated Recurrent Unit (GRU) via Back Propagation Through
 *  Time (BPTT).
 *  val (x, y) = getTrainingData (vocabulary_size, sentence_size)
 *  @param x  the input sequence/time series
 *  @param y  the output sequence/time series
 */
class GRU (x: MatrixD, y: MatrixD):

    // set GRU and data scale
    private val iMem_size       = 4 
    private val vocabulary_size = x.dim            // e.g., 64, number of distinct words
    private val sentence_size   = x.dim2           // e.g., 20, number of words in a sentence (including start and end symbol)
                                                   // since we will only use one sentence for training,
                                                   // this is also the total steps during training.

    private val _1 = VectorD.one (iMem_size)       // vector of all ones for e.g., 1 - z

    // initialize parameters (weights and biases)
    // multiplier for input x_t of intermediate variables
    // note: Matlab rand -> NormalMat
    private val rmg1 = NormalMat (iMem_size, vocabulary_size, 0.0, 0.01)
    private val U_z  = rmg1.gen
    private val U_r  = rmg1.gen
    private val U_c  = rmg1.gen

    // multiplier for previous s of intermediate variables
    private val rmg2 = NormalMat (iMem_size, iMem_size, 0.0, 0.01)
    private val W_z  = rmg2.gen
    private val W_r  = rmg2.gen
    private val W_c  = rmg2.gen

    // bias terms of intermediate variables - converted to VectorD
    private val rvg1 = NormalVec_c (iMem_size, 0.0, 0.01)
    private val b_z  = rvg1.gen
    private val b_r  = rvg1.gen
    private val b_c  = rvg1.gen

    // decoder for generating output
    private val rmg3 = NormalMat (vocabulary_size, iMem_size, 0.0, 0.01)
    private val rvg3 = NormalVec_c (vocabulary_size, 0.0, 0.01)
    private val V    = rmg3.gen
    private val b_V  = rvg3.gen                                          // bias of decoder - converted to Vector

    // previous s of step 1
    private val s_0 = rvg1.gen                                           // converted to vector

    private val max_epochs = 2                                           // maximum number of iterations
    private val L = new VectorD (sentence_size)                          // store loss function values
    println (s"L = $L")

    // initialize results
    // Matlab: zeros -> new MatrixD
    private val s     = new MatrixD (iMem_size, sentence_size)           // hidden state
    private val y_hat = new MatrixD (vocabulary_size, sentence_size)     // predicted output
    private val z     = new MatrixD (iMem_size, sentence_size)           // update gate
    private val r     = new MatrixD (iMem_size, sentence_size)           // reset gate
    private val c     = new MatrixD (iMem_size, sentence_size)           // candidate state

    // the partial derivative of weights and biases
    private var ds_0 = new VectorD (s_0.dim)
    private var dU_c = new MatrixD (U_c.dim, U_c.dim2)
    private var dU_r = new MatrixD (U_r.dim, U_r.dim2)
    private var dU_z = new MatrixD (U_z.dim, U_z.dim2)
    private var dW_c = new MatrixD (W_c.dim, W_c.dim2)
    private var dW_r = new MatrixD (W_r.dim, W_r.dim2)
    private var dW_z = new MatrixD (W_z.dim, W_z.dim2)

    private var db_z = new VectorD (b_z.dim)
    private var db_r = new VectorD (b_r.dim)
    private var db_c = new VectorD (b_c.dim)

    private var db_V: VectorD = null
    private var dV   = new MatrixD (V.dim, V.dim2)

    private val eta  = 0.1                                               // the learning rate

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Train the GRU.
     */
    def train (): Unit =

        for it <- 1 to max_epochs do
            // forward propagate: get the intermediate and output results
            forward ()

            println (s"train: for epoch $it: loss function L = $L")

            // back propogate: calculate the gradient (the partial derivatives)
            backward ()

            // update the parameters (weights and biases)
            update_params ()
        end for
    end train

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Forward propagate calculates, y_hat, loss and intermediate variables for each step.
     */
    def forward (): Unit =

        // calculate result for step 1 since s_0 is not in s
        // note: Matlab row wild-card : becomes ?
        // note: Matlab starts at 1, Scala at 0

        z(?, 0) = sigmoid_ (U_z * x(?, 0) + W_z * s_0 + b_z)
        r(?, 0) = sigmoid_ (U_r * x(?, 0) + W_r * s_0 + b_r)
        c(?, 0) = tanh_ (U_c * x(?, 0) + W_c * (s_0 * r(?, 0) ) + b_c)
        s(?, 0) = (_1 - z(?, 0)) * c(?, 0) + z(?, 0) * s_0
        y_hat(?, 0) = softmax_ (V * s(?, 0) + b_V)
        L(0) = (-y(?, 0) * log_ (y_hat(?, 0))).sum

        // calculate results for step 2 − sentence_size similarly (i-th word)
        // note Matlab element-wise multiplication .* becomes *

        for word <- 1 until sentence_size do
            z(?, word) = sigmoid_ (U_z * x(?, word) + W_z * s(?, word-1) + b_z)
            r(?, word) = sigmoid_ (U_r * x(?, word) + W_r * s(?, word-1) + b_r)
            c(?, word) = tanh_ (U_c * x(?, word) + W_c * (s(?, word-1) * r(?, word)) + b_c) 
            s(?, word) = (_1 - z(?, word)) * c(?, word) + z(?, word) * s(?, word-1)
            y_hat(?, word) = softmax_ (V * s(?, word) + b_V)
            L(word) = (-y(?, word) * log_ (y_hat(?, word))).sum
        end for
    end forward

    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Backward propagate to calculate gradient using chain rule (O(sentence_size) time)
     */
    def backward (): Unit =

        // calculate gradient using chain rule
        // note Matlab: A' is A.transpose
        // note Matlab: sum (delta_y, 2) returns the row sums of matrix delta_y
        // note Matlab: delta_y(?, word) * s(?, word)' -> delta_y(?, word) outer s(?, word)
        val delta_y = y_hat - y
        db_V    = delta_y.sumVr
//      dV      = new MatrixD (V.dim, V.dim2)
        for word <- 0 until sentence_size do
            dV += outer (delta_y(?, word), s(?, word))                  // outer vector product
        end for

        val ds_single = V.transpose * delta_y

        // calculate the derivative contribution of each step and add them up
        var ds_cur = new VectorD (ds_single.dim)
        for word <- sentence_size-1 to 1 by -1 do
            ds_cur += ds_single(?, word)
            val ds_cur_bk = ds_cur

            // mix for new state
            val dtanhInput = (ds_cur * (_1 - z(?, word)) * (_1 - c(?, word) * c(?, word)))
            db_c   += dtanhInput
            dU_c   += outer (dtanhInput, x(?, word))
            dW_c   += outer (dtanhInput, (s(?, word-1) * r(?, word)))
            val dsr = W_c.transpose * dtanhInput
            ds_cur  = dsr * r(?, word)

            // reset gate
            val dsigInput_r = dsr * s(?, word-1) * r(?, word) * (_1 - r(?, word))
            db_r   += dsigInput_r
            dU_r   += outer (dsigInput_r, x(?, word))
            dW_r   += outer (dsigInput_r, s(?, word-1))
            ds_cur += W_r.transpose * dsigInput_r
            ds_cur += ds_cur_bk * z(?, word)
            val dz  = ds_cur_bk * (s(?, word-1) - c(?, word))

            // update gate
            val dsigInput_z = dz * z(?, word) * (_1 - z(?, word))
            db_z   += dsigInput_z
            dU_z   += outer (dsigInput_z, x(?, word))
            dW_z   += outer (dsigInput_z, s(?, word-1))
            ds_cur += W_z.transpose * dsigInput_z
        end for

        // case: s_1 -> s_0
        ds_cur += ds_single(?, 0)

        val dtanhInput = (ds_cur * (_1 - z(?, 0)) * (_1 - c(?, 0) * c(?, 0)))
        db_c += dtanhInput
        dU_c += outer (dtanhInput, x(?, 0))
        dW_c += outer (dtanhInput, (s_0 * r(?, 0)))
        val dsr = W_c.transpose * dtanhInput
        ds_0 += dsr * r(?, 0)

        val dsigInput_r = dsr * s_0 * r(?, 0) * (_1 - r(?, 0))
        db_r  += dsigInput_r
        dU_r  += outer (dsigInput_r, x(?, 0))
        dW_r  += outer (dsigInput_r, s_0)
        ds_0  += W_r.transpose * dsigInput_r
        ds_0  += ds_cur * z(?, 0)
        val dz = ds_cur * (s_0 - c(?, 0))

        val dsigInput_z = dz * z(?, 0) * (_1 - z(?, 0))
        db_z += dsigInput_z
        dU_z += outer (dsigInput_z, x(?, 0))
        dW_z += outer (dsigInput_z, s_0)
        ds_0 += W_z.transpose * dsigInput_z
    end backward
   
    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Based on the calculated partial derivatives, update the parameters (weights
     *  and biases).
     */
    def update_params (): Unit =
        println ("To Be Implemented")
    end update_params

end GRU


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** Create a fake training dataset:  generate only one sentence for training.
 *  Only for testing. Needs to be changed to read in training data from files.
 *  The words are one-hot encoded into a column vector.
 */
def getTrainingData (vocabulary_size: Int, sentence_size: Int): (MatrixD, MatrixD) =

    import scalation.random.Randi

    assert (vocabulary_size > 2)                                     // for start and end of sentence symbols
    assert (sentence_size > 0)

    // define start and end of sentence in the vocabulary
    val SENTENCE_START = new VectorD (vocabulary_size)               // start: [1, 0, 0, ...]
    SENTENCE_START(0) = 1
    val SENTENCE_END = new VectorD (vocabulary_size)                 // end:   [0, 1, 0, ...]
    SENTENCE_END(1) = 1

    println (s"SENTENCE_START = $SENTENCE_START")
    println (s"SENTENCE_END   = $SENTENCE_END")

    // generate sentence
    val i_ran = Randi (0, vocabulary_size-3)                         // random integer generator
    val z_t = new MatrixD (vocabulary_size, sentence_size-1)         // leave one slot for SENTENCE START
    for word <- 0 until sentence_size-1 do
        // generate a random word excludes start and end symbol
        z_t(i_ran.igen+2, word) = 1                                  // set a 1 in position to indicate a word
    end for

    val x_t = SENTENCE_START +^: z_t                                 // training input matrix (prepend vector)
    val y_t = z_t :^+ SENTENCE_END                                   // training output matrix (append vector)

    (x_t, y_t)
end getTrainingData


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `gRUTest` main function tests the `GRU` class.
 *  > runMain scalation.modeling.forecasting.gRUTest
 */
@main def gRUTest (): Unit =

    val vocabulary_size = 5
    val sentence_size   = 8

    val (x_t, y_t) = getTrainingData (vocabulary_size, sentence_size)

    println (s"x_t = $x_t")
    println (s"y_t = $y_t")

    banner ("Create a Gated Recurrent Unit (GRU)")
    val mod = new GRU (x_t, y_t)
    mod.train ()
//  mod.test ()

end gRUTest

