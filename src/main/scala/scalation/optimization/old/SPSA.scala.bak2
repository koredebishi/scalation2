
//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  Yulong Wang
 *  @version 2.0
 *  @date    Thursday Feb 17 13:32:52 EDT 2022
 *  @see     LICENSE (MIT style license file).
 *
 *  @title   Simultaneous Perturbation Stochastic Approximation
 */

package scalation
package optimization

import scala.math.pow

import scalation.mathstat.{FunctionV2S, VectorD}
import scalation.random.{Bernoulli, Normal}

//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `SPSA` class implements the Simultaneous Perturbation Stochastic Approximation
 *  algorithm for rough approximation of gradients.
 *  @see https://www.jhuapl.edu/spsa/PDF-SPSA/Matlab-SPSA_Alg.pdf
 *  @param f       the vector to scalar function whose approximate gradient is sought
 *  @param theta   the current point f(theta)
 *  @param nSteps  the number of steps
 */
class SPSA (f: FunctionV2S, theta: VectorD, nSteps: Int = 20):
//    extends Minimizer:

    private val alpha = 0.602
    private val gamma = 0.101
    private val A     = 100
    private val a     = 0.16       // these numbers are from Spall (1998) DOI: 10.1109/7.705889
    private val c     = 1
    private val bernoully = Bernoulli (0.5, 5)

    var x_best = theta             // best theta or parameter to get the lowest error from loss function
    var f_best = Double.MaxValue
    var x = theta.copy             // use x for theta copy by value

    def basic (): Double =
        for k <- 0 to nSteps do
            val ak = a / pow (A + k + 1, alpha)
            val ck = c / pow (k + 1, gamma)
            val delta   = 2 * bernoully.igen - 1       // -1 or 1
            val x_plus  = x + ck * delta
            val x_minus = x - ck * delta
            val y_plus  = f (x_plus)
            val y_minus = f (x_minus)
            val ghat = (y_plus - y_minus) / (2 * ck * delta)
            x -= ak * ghat
            if f(x) < f_best then
                x_best = x.copy                        // copy by value
                f_best = f(x_best)
            end if
        end for

        println (s"x_last is $x and y(x_last) at the end is ${f(x)} and \n " +
                 s"lowest is $x_best and $f_best")
        f_best
    end basic

end SPSA


//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `sPSATest` main function tests the SPSA class.
 *  > runMain scalation.optimization.sPSATest
 */
@main def sPSATest (): Unit =

    banner ("Minimize: (x_0 - 3)^2 + (x_1 - 4)^2 + 1")
    val noise = Normal (0, 0.1)

    def f (x: VectorD): Double = (x(0) - 3)~^2 + (x(1) - 4)~^2 + 1 + noise.gen

    val x = VectorD (1, 2)
    val optimizer = new SPSA (f, x, 1200)
    val opt = optimizer.basic ()
//  val opt = optimizer.fastconvergence2 ()
    println (s"][ optimal solution (f(x), x) = $opt")

end sPSATest

