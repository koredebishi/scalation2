
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  John Miller, Sainagesh Veeravalli
 *  @version 2.0
 *  @date    Thu May 11 15:38:07 EDT 2023
 *  @see     LICENSE (MIT style license file).
 *
 *  @title   Gated Recurrent Unit (GRU) for Multivariate Time Series
 * 
 *  Translated from Matlab to Scala
 *  @see     https://www.math.ucla.edu/~minchen/doc/BPTTTutorial.pdf
 */

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/* This Matlab program tests the BPTT process we manually developed for GRU.
 * We calculate the gradients of GRU parameters with chain rule, and then
 * compare them to the numerical gradients to check whether our chain rule
 * derivation is correct.
 *
 * Here, we provided 2 versions of BPTT, backward_direct() and backward().
 * The former one is the direct idea to calculate gradient within each step
 * and add them up (O(seq_sizeˆ2) time) . The latter one is optimized to
 * calculate the contribution of each step to the overall gradient, which is
 * only O(seq_size) time.
 *
 * This is very helpful for people who want to implement GRU in Caffe since
 * Caffe does not support auto−differentiation. This is also very helpful for
 * the people who want to know the details about Back-propagation Through
 * Time algorithm in the Recurrent Neural Networks (such as GRU and LSTM)
 * and also get a sense on how auto−differentiation is possible.
 *
 * NOTE: We does not involve SGD training here. With SGD training, this
 * program would become a complete implementation of GRU which can be
 * trained with sequence data. However, since this is only a CPU serial
 * Matlab version of GRU, applying it on large datasets will be dramatically
 * slow.
 *
 * Matlab code:
 * by Minchen Li, at The University of British Columbia. 2016−04−21
 */

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** Standard RNN (GRU/LSTM) Formats for Multivariate Time Series Data:
 *  Matlab:
 *
 *  Keras: 3D format expected by GRU/LSTM is [samples, timesteps, features].
 *         => indexing [timestamp t, lags k, variable j]
 *  PyTorch:
 */

package scalation
package modeling
package forecasting

import scala.math.log

import scalation.mathstat.{MatrixD, VectorD}
import scalation.random.{NormalMat, NormalVec_c}

import ActivationFun.{sigmoid_, softmax_, tanh_}
import MatrixD.outer

def log_ (x: VectorD): VectorD = x.map (log)

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `CRU` class implements Gated Recurrent Unit (GRU) via Back Propagation Through
 *  Time (BPTT).  At each time point x_t, there is a vector representing several variables
 *  or the encoding of a word.
 *  Time series: (x_t: t = 0, 1, ..., seq_size-1) where seq_size is the number of time points/words
 *  @param x  the input sequence/time series
 *  @param y  the output sequence/time series
 */
class GRU (x: MatrixD, y: MatrixD):

    // set GRU and data scale
    private val mem_size   = 4                     // memory size for hidden state
    private val vocab_size = x.dim                 // e.g., 64, number of variable or distinct words
    private val seq_size   = x.dim2                // e.g., 20, number of words in a sentence (including start and end symbol)
                                                   // since we will only use one sentence for training,
                                                   // this is also the total steps during training.

    private val _1 = VectorD.one (mem_size)        // vector of all ones for e.g., 1 - z

    // initialize parameters (weights and biases)
    // multiplier for input x_t of intermediate variables
    // note: Matlab rand -> NormalMat or NormalVec_c
    private val rmg1 = NormalMat (mem_size, vocab_size, 0.0, 0.01)
    private var U_z  = rmg1.gen
    private var U_r  = rmg1.gen
    private var U_c  = rmg1.gen

    // multiplier for previous s of intermediate variables
    private val rmg2 = NormalMat (mem_size, mem_size, 0.0, 0.01)
    private var W_z  = rmg2.gen
    private var W_r  = rmg2.gen
    private var W_c  = rmg2.gen

    // bias terms of intermediate variables - converted to VectorD
    private val rvg1 = NormalVec_c (mem_size, 0.0, 0.01)
    private var b_z  = rvg1.gen
    private var b_r  = rvg1.gen
    private var b_c  = rvg1.gen

    // decoder for generating output
    private val rmg3 = NormalMat (vocab_size, mem_size, 0.0, 0.01)
    private val rvg3 = NormalVec_c (vocab_size, 0.0, 0.01)
    private var V    = rmg3.gen
    private var b_V  = rvg3.gen                                    // bias of decoder - converted to Vector

    // previous s of step 1
    private var s_0 = rvg1.gen                                     // converted to vector

    private val max_epochs = 20                                    // maximum number of iterations
    private val L = new VectorD (seq_size)                         // store loss function values
    println (s"L = $L")

    // initialize results
    // Matlab: zeros -> new MatrixD
    private val s  = new MatrixD (mem_size, seq_size)              // hidden state (change s -> h)
    private val yp = new MatrixD (vocab_size, seq_size)            // predicted output
    private val z  = new MatrixD (mem_size, seq_size)              // update gate
    private val r  = new MatrixD (mem_size, seq_size)              // reset gate
    private val c  = new MatrixD (mem_size, seq_size)              // candidate state

    // the partial derivative of weights and biases
    private var ds_0 = new VectorD (s_0.dim)
    private var dU_c = new MatrixD (U_c.dim, U_c.dim2)
    private var dU_r = new MatrixD (U_r.dim, U_r.dim2)
    private var dU_z = new MatrixD (U_z.dim, U_z.dim2)
    private var dW_c = new MatrixD (W_c.dim, W_c.dim2)
    private var dW_r = new MatrixD (W_r.dim, W_r.dim2)
    private var dW_z = new MatrixD (W_z.dim, W_z.dim2)

    private var db_z = new VectorD (b_z.dim)
    private var db_r = new VectorD (b_r.dim)
    private var db_c = new VectorD (b_c.dim)

    private var db_V: VectorD = null
    private var dV   = new MatrixD (V.dim, V.dim2)

    private val eta  = 0.25                                        // the learning rate

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Train the GRU using simple gradient descent.
     */
    def train (): Unit =

        for it <- 1 to max_epochs do
            // forward propagate: get the intermediate and output results
            forward ()

            println (s"train: for epoch $it: loss function L = $L")
            println(s"train: for epoch $it: total loss function L.sum = ${L.sum}")

            // back propogate: calculate the gradient (the partial derivatives)
            backward ()

            // update the parameters (weights and biases)
            update_params ()
        end for
    end train

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Forward propagate calculates, yp, loss and intermediate variables for each step.
     */
    def forward (): Unit =

        // calculate result for step 1 since s_0 is not in s
        // note: Matlab: row wild-card : becomes ?
        // note: Matlab: starts at 1, Scala at 0

        z(?, 0)  = sigmoid_ (U_z * x(?, 0) + W_z * s_0 + b_z)
        r(?, 0)  = sigmoid_ (U_r * x(?, 0) + W_r * s_0 + b_r)
        c(?, 0)  = tanh_ (U_c * x(?, 0) + W_c * (s_0 * r(?, 0) ) + b_c)
        s(?, 0)  = (_1 - z(?, 0)) * c(?, 0) + z(?, 0) * s_0
        yp(?, 0) = softmax_ (V * s(?, 0) + b_V)
        L(0)     = (-y(?, 0) * log_ (yp(?, 0))).sum

        // calculate results for step 2 − seq_size similarly (t-th word)
        // note Matlab element-wise multiplication .* becomes *

        for t <- 1 until seq_size do
            z(?, t)  = sigmoid_ (U_z * x(?, t) + W_z * s(?, t-1) + b_z)
            r(?, t)  = sigmoid_ (U_r * x(?, t) + W_r * s(?, t-1) + b_r)
            c(?, t)  = tanh_ (U_c * x(?, t) + W_c * (s(?, t-1) * r(?, t)) + b_c) 
            s(?, t)  = (_1 - z(?, t)) * c(?, t) + z(?, t) * s(?, t-1)
            yp(?, t) = softmax_ (V * s(?, t) + b_V)
            L(t)     = (-y(?, t) * log_ (yp(?, t))).sum
        end for
    end forward

    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Backward propagate to calculate gradient using chain rule (O(seq_size) time).
     */
    def backward (): Unit =

        // calculate gradient using chain rule
        // note Matlab: A' is A.transpose
        // note Matlab: sum (delta_y, 2) returns the row sums of matrix delta_y
        // note Matlab: delta_y(?, word) * s(?, word)' -> delta_y(?, t) outer s(?, t)

        val delta_y = yp - y
        db_V    = delta_y.sumVr
//      dV      = new MatrixD (V.dim, V.dim2)
        for t <- 0 until seq_size do dV += outer (delta_y(?, t), s(?, t))   // outer vector product

        val ds_single = V.transpose * delta_y

        // calculate the derivative contribution of each step and add them up
        var ds_cur = new VectorD (ds_single.dim)
        for t <- seq_size-1 to 1 by -1 do
            ds_cur += ds_single(?, t)
            val ds_cur_bk = ds_cur

            // mix for new state
            val dtanhIn = (ds_cur * (_1 - z(?, t)) * (_1 - c(?, t) * c(?, t)))
            db_c   += dtanhIn
            dU_c   += outer (dtanhIn, x(?, t))
            dW_c   += outer (dtanhIn, (s(?, t-1) * r(?, t)))
            val dsr = W_c.transpose * dtanhIn
            ds_cur  = dsr * r(?, t)

            // reset gate (r)
            val dsigIn_r = dsr * s(?, t-1) * r(?, t) * (_1 - r(?, t))
            db_r   += dsigIn_r
            dU_r   += outer (dsigIn_r, x(?, t))
            dW_r   += outer (dsigIn_r, s(?, t-1))
            ds_cur += W_r.transpose * dsigIn_r
            ds_cur += ds_cur_bk * z(?, t)
            val dz  = ds_cur_bk * (s(?, t-1) - c(?, t))

            // update gate (z)
            val dsigIn_z = dz * z(?, t) * (_1 - z(?, t))
            db_z   += dsigIn_z
            dU_z   += outer (dsigIn_z, x(?, t))
            dW_z   += outer (dsigIn_z, s(?, t-1))
            ds_cur += W_z.transpose * dsigIn_z
        end for

        // case: s_1 -> s_0
        ds_cur += ds_single(?, 0)

        val dtanhIn = (ds_cur * (_1 - z(?, 0)) * (_1 - c(?, 0) * c(?, 0)))
        db_c += dtanhIn
        dU_c += outer (dtanhIn, x(?, 0))
        dW_c += outer (dtanhIn, (s_0 * r(?, 0)))
        val dsr = W_c.transpose * dtanhIn
        ds_0 += dsr * r(?, 0)

        val dsigIn_r = dsr * s_0 * r(?, 0) * (_1 - r(?, 0))
        db_r  += dsigIn_r
        dU_r  += outer (dsigIn_r, x(?, 0))
        dW_r  += outer (dsigIn_r, s_0)
        ds_0  += W_r.transpose * dsigIn_r
        ds_0  += ds_cur * z(?, 0)
        val dz = ds_cur * (s_0 - c(?, 0))

        val dsigIn_z = dz * z(?, 0) * (_1 - z(?, 0))
        db_z += dsigIn_z
        dU_z += outer (dsigIn_z, x(?, 0))
        dW_z += outer (dsigIn_z, s_0)
        ds_0 += W_z.transpose * dsigIn_z
    end backward
   
    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Based on the calculated partial derivatives, update the parameters (weights
     *  and biases).
     */
    def update_params (): Unit =
        U_z -= dU_z * eta
        U_r -= dU_r * eta
        U_c -= dU_c * eta
        W_z -= dW_z * eta
        W_r -= dW_r * eta
        W_c -= dW_c * eta
        b_z -= db_z * eta
        b_r -= db_r * eta
        b_c -= db_c * eta
        V   -= dV * eta
        b_V -= db_V * eta
    end update_params

end GRU


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** Generate a fake sequence dataset:  generate only one sentence for training.
 *  Only for testing. Needs to be changed to read in training data from files.
 *  The words are one-hot encoded into a column vector.
 *  @param vocab_size  the number of variables/word encoding size
 *  @param seq_size    the sequence size (number of time points/words)
 */
def genSequenceData (vocab_size: Int, seq_size: Int): (MatrixD, MatrixD) =

    import scalation.random.Randi

    assert (vocab_size > 2)                                        // for start and end of sentence symbols
    assert (seq_size > 0)

    // define start and end of sentence in the vocabulary
    val SENTENCE_START = new VectorD (vocab_size)                  // start: [1, 0, 0, ...]
    SENTENCE_START(0) = 1
    val SENTENCE_END = new VectorD (vocab_size)                    // end:   [0, 1, 0, ...]
    SENTENCE_END(1) = 1

    println (s"SENTENCE_START = $SENTENCE_START")
    println (s"SENTENCE_END   = $SENTENCE_END")

    // generate sentence
    val i_ran = Randi (0, vocab_size-3)                            // random integer generator
    val z_t = new MatrixD (vocab_size, seq_size-1)                 // leave one slot for SENTENCE START
    for t <- 0 until seq_size-1 do
        // generate a random word excludes start and end symbol
        z_t(i_ran.igen+2, t) = 1                                   // set a 1 in position to indicate a word
    end for

    val x_t = SENTENCE_START +^: z_t                               // training input matrix (prepend vector)
    val y_t = z_t :^+ SENTENCE_END                                 // training output matrix (append vector)

    (x_t, y_t)
end genSequenceData


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `gRUTest` main function tests the `GRU` class on randomly generated
 *  sequence data meant to represent encoded words 
 *  > runMain scalation.modeling.forecasting.gRUTest
 */
@main def gRUTest (): Unit =

    val vocab_size = 5
    val seq_size   = 8

    val (x_t, y_t) = genSequenceData (vocab_size, seq_size)

    println (s"x_t = $x_t")
    println (s"y_t = $y_t")

    banner ("Create a Gated Recurrent Unit (GRU)")
    val mod = new GRU (x_t, y_t)
    mod.train ()
//  mod.test ()

end gRUTest


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `gRUTest2` main function tests the `GRU` class on sequence data read as words
 *  in a file that encoded and pass into `GRU`
 *  > runMain scalation.modeling.forecasting.gRUTest2
 */
@main def gRUTest2 (): Unit =

    println ("read words from a text file")

end gRUTest2


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `gRUTest3` main function tests the `GRU` class on sequence data corresponding
 *  to multivariate time series data
 *  in a file that encoded and pass into `GRU`
 *  > runMain scalation.modeling.forecasting.gRUTest3
 */
@main def gRUTest3 (): Unit =

    println ("read multivariate time series from a CSV file")

end gRUTest3

